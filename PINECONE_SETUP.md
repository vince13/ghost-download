# Pinecone RAG Integration Setup Guide

This guide will help you set up Pinecone for the Ghost knowledge base RAG (Retrieval-Augmented Generation) system.

## Overview

The RAG system allows Ghost to:
1. **Store** knowledge base documents as vector embeddings in Pinecone
2. **Retrieve** relevant context from your documents during live calls
3. **Enhance** coaching cues with information from your playbooks and battlecards

## Prerequisites

1. **Pinecone Account**: Sign up at [pinecone.io](https://www.pinecone.io)
   - Pinecone is a **vector database** (stores embeddings)
   - Pinecone does NOT generate embeddings - you need a separate service

2. **Embedding Service** (REQUIRED - separate from Pinecone): Choose one:
   - **Hugging Face** (Free tier available) - Recommended for development
   - **OpenAI** (Paid, better quality) - Recommended for production
   
   **Important**: Pinecone only stores vectors. You must use Hugging Face or OpenAI to generate the embeddings first.

## Step 1: Create Pinecone Index

1. Log in to [Pinecone Console](https://app.pinecone.io)
2. Create a new index with these settings:
   - **Name**: `ghost-knowledge-base` (or your preferred name)
   - **Dimensions**: `384` (for Hugging Face all-MiniLM-L6-v2) or `1536` (for OpenAI text-embedding-3-small)
   - **Metric**: `cosine`
   - **Pod Type**: `s1.x1` (free tier) or higher for production
3. Note your **Index Name** and **API Key**

## Step 2: Configure Environment Variables

Add these to your Vercel environment variables (or `.env.local` for local development):

### Required - Pinecone (Vector Database):
```bash
PINECONE_API_KEY=your-pinecone-api-key
PINECONE_INDEX_NAME=ghost-knowledge-base
```

### Required - Embedding Service (Separate from Pinecone):
**You MUST configure one of these** - Pinecone does not generate embeddings:

**Option A: Hugging Face (Free)**
```bash
HUGGINGFACE_API_KEY=your-huggingface-api-key
```

**Option B: OpenAI (Better Quality)**
```bash
OPENAI_API_KEY=your-openai-api-key
```

## Step 3: Install Pinecone Package

The Pinecone package should be installed automatically. If not, run:

```bash
npm install @pinecone-database/pinecone
```

## Step 4: Test the Integration

1. **Upload a document** to the Knowledge Base in the Ghost app
2. **Check the document status** - it should progress: `pending` â†’ `chunking` â†’ `indexed`
3. **Start a call** and mention something from your uploaded document
4. **Verify** that coaching cues reference your document content

## How It Works

### Architecture Overview:
```
Document â†’ Embedding Service (Hugging Face/OpenAI) â†’ Pinecone (Vector DB) â†’ Retrieval â†’ LLM
```

**Key Point**: Pinecone is ONLY for storage. Embeddings are generated by Hugging Face or OpenAI.

### Document Upload Flow:
1. User uploads a document (text file, markdown, etc.)
2. Document is **chunked** into semantic pieces (~500 words each)
3. Each chunk is **embedded** using Hugging Face or OpenAI API (NOT Pinecone)
4. Embeddings (vectors) are **stored** in Pinecone with user namespace
5. Document status updates to `indexed`

### RAG Retrieval Flow:
1. During a call, transcript is processed
2. Query text is **embedded** using Hugging Face or OpenAI (same model as storage)
3. Pinecone is **queried** with the query embedding to find top 5 most similar chunks
4. Relevant context (original text chunks) is **injected** into the LLM prompt
5. LLM generates coaching cues **grounded** in your knowledge base

## Namespace Strategy

Each user's documents are stored in a separate Pinecone namespace:
- Format: `user-{userId}`
- Example: `user-abc123xyz`

This ensures:
- âœ… Data isolation between users
- âœ… Easy cleanup when users delete documents
- âœ… Scalable multi-tenant architecture

## Troubleshooting

### Document Status Stuck on "chunking"
- Check Vercel function logs for errors
- Verify `PINECONE_API_KEY` and `PINECONE_INDEX_NAME` are set
- Check embedding service API key is valid

### No RAG Context Retrieved
- Verify documents are indexed (status = "indexed")
- Check Pinecone index dimensions match embedding model
- Review `/api/retrieve-rag` logs in Vercel

### Embedding Generation Fails
- **Important**: Pinecone does NOT generate embeddings
- Verify `HUGGINGFACE_API_KEY` or `OPENAI_API_KEY` is set (REQUIRED)
- Check API rate limits (Hugging Face free tier has limits)
- Try switching to OpenAI for production
- Get Hugging Face API key: https://huggingface.co/settings/tokens
- Get OpenAI API key: https://platform.openai.com/api-keys

## Cost Considerations

### Pinecone:
- **Free Tier**: 1 index, 100K vectors, 1 pod
- **Paid**: Starts at ~$70/month for production workloads

### Embeddings:
- **Hugging Face**: Free tier (rate-limited), paid plans available
- **OpenAI**: ~$0.02 per 1M tokens (very affordable)

### Estimated Monthly Cost:
- **Small team** (10 users, 50 docs): ~$5-10/month
- **Medium team** (100 users, 500 docs): ~$50-100/month
- **Large team** (1000+ users): Custom pricing

## Next Steps

1. âœ… Set up Pinecone index
2. âœ… Configure environment variables
3. âœ… Upload test documents
4. âœ… Verify RAG retrieval works
5. ðŸš€ Start using enhanced coaching cues!

## Support

If you encounter issues:
1. Check Vercel function logs
2. Review Pinecone console for index status
3. Verify all environment variables are set correctly
4. Test embedding generation independently

